---
title: 'Animal nutrition journal report: November 2017'
author: "ruminoreticulum@gmail.com"
date: "12/01/2017"
output:
html_document:
df_print: paged
fig_height: 6
highlight: textmate
theme: default
toc: no
toc_depth: 3
toc_float: yes
---
```{r setup, echo=FALSE, include=FALSE}
options(warn=-1)
library(knitr)
library(ggplot2)
library(dplyr)
library(tm)
library(SnowballC)
library(wordcloud)
library(RColorBrewer)
library(stringr)
theme_set(theme_bw(base_family="AppleGothic"))
```
A report is published for **a personal interesting** of the recent animal nutrition. The papers about *"animal nutrition"* which published at some SCI(E) journals were selected and analyzed. HTML scraping, data visualization, and report generation are automate process except sorting of the papers. Codes are available at https://github.com/YoungjunNa/journal_scraping
# 1. List of journals
```{r echo=TRUE}
journal_list <- read.csv("journal_list.txt", header=TRUE)
kable(journal_list)
```
# 2. List of papers
## 2.1. Review
```{r echo=TRUE}
results <- read.csv("journal_result.txt", header=TRUE)
results <- results[,c(7,8,5,4,1,2,3,6)]
filter(results, type=="review")
```
## 2.2. Monogastric
```{r rows.print=100, echo=TRUE}
filter(results, type=="original article" & class=="monogastric")
```
## 2.3. Ruminant
```{r rows.print=100, echo=TRUE}
filter(results, type=="original article" & class=="ruminant")
```
# 3. Wordcloud analysis
## 3.1. Monogastric
```{r cols.print=6, rows.print=18, echo=FALSE}
wc <- read.csv("journal_result.txt", stringsAsFactors = FALSE)
#wc <- filter(wc, class == "ruminant")
wc <- filter(wc, class == "monogastric")
#wc <- filter(wc, str_detect(wc$subject,"supplement")==TRUE)
#WORDCLOUD
wc <- Corpus(VectorSource(wc$subject))
wc_data<-tm_map(wc,stripWhitespace)
wc_data<-tm_map(wc_data, removeWords, c("and"))
wc_data<-tm_map(wc_data, tolower)
wc_data<-tm_map(wc_data,removeNumbers)
wc_data<-tm_map(wc_data, removePunctuation)
wc_data<-tm_map(wc_data, removeWords, stopwords("english"))
wc_data<-tm_map(wc_data, removeWords, c("affect","effect","effects","and","the","our","that","for","are","also","more","has","must","have","should","this","with"))
tdm_wc<-TermDocumentMatrix(wc_data) #Creates a TDM
TDM1<-as.matrix(tdm_wc) #Convert this into a matrix format
v = sort(rowSums(TDM1), decreasing = TRUE) #Gives you the frequencies for every word
wordcloud(wc_data, max.words = Inf, min.freq = 1, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
## Term-Document
dtm <- TermDocumentMatrix(wc_data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
```
## 3.2. Ruminant
```{r cols.print=6, rows.print=18, echo=FALSE}
wc <- read.csv("journal_result.txt", stringsAsFactors = FALSE)
wc <- filter(wc, class == "ruminant")
#wc <- filter(wc, class == "monogastric")
#wc <- filter(wc, str_detect(wc$subject,"supplement")==TRUE)
#WORDCLOUD
wc <- Corpus(VectorSource(wc$subject))
wc_data<-tm_map(wc,stripWhitespace)
wc_data<-tm_map(wc_data, removeWords, c("and"))
wc_data<-tm_map(wc_data, tolower)
wc_data<-tm_map(wc_data,removeNumbers)
wc_data<-tm_map(wc_data, removePunctuation)
wc_data<-tm_map(wc_data, removeWords, stopwords("english"))
wc_data<-tm_map(wc_data, removeWords, c("cattle","affect","effect","effects","and","the","our","that","for","are","also","more","has","must","have","should","this","with"))
tdm_wc<-TermDocumentMatrix(wc_data) #Creates a TDM
TDM1<-as.matrix(tdm_wc) #Convert this into a matrix format
v = sort(rowSums(TDM1), decreasing = TRUE) #Gives you the frequencies for every word
wordcloud(wc_data, max.words = Inf, min.freq = 1, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
## Term-Document
dtm <- TermDocumentMatrix(wc_data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
```
## 3.3. Overall
```{r cols.print=6, rows.print=18, echo=FALSE}
wc <- read.csv("journal_result.txt", stringsAsFactors = FALSE)
#wc <- filter(wc, class == "ruminant")
#wc <- filter(wc, class == "monogastric")
#wc <- filter(wc, str_detect(wc$subject,"supplement")==TRUE)
#WORDCLOUD
wc <- Corpus(VectorSource(wc$subject))
wc_data<-tm_map(wc,stripWhitespace)
wc_data<-tm_map(wc_data, removeWords, c("and"))
wc_data<-tm_map(wc_data, tolower)
wc_data<-tm_map(wc_data,removeNumbers)
wc_data<-tm_map(wc_data, removePunctuation)
wc_data<-tm_map(wc_data, removeWords, stopwords("english"))
wc_data<-tm_map(wc_data, removeWords, c("affect","effect","effects","and","the","our","that","for","are","also","more","has","must","have","should","this","with"))
tdm_wc<-TermDocumentMatrix(wc_data) #Creates a TDM
TDM1<-as.matrix(tdm_wc) #Convert this into a matrix format
v = sort(rowSums(TDM1), decreasing = TRUE) #Gives you the frequencies for every word
wordcloud(wc_data, max.words = Inf, min.freq = 1, random.order = FALSE, rot.per = 0.1, colors = brewer.pal(8, "Dark2"))
## Term-Document
dtm <- TermDocumentMatrix(wc_data)
m <- as.matrix(dtm)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
d
```
library(ggplot2)
install.packages("ggplot2")
install.packages("dplyr")
install.packages("tm")
install.packages("SnowballC")
install.packages("stringr")
install.packages("stringr")
getwd()
getwd()
read.csv("journal_result_2017_12.txt")
result <- read.csv("journal_result_2017_12.txt")
View(result)
#.Renviron
GL_AUTH="C:/Users/Youngjun/Google Drive/Google_cloud/translateR-079f74265489.json"
gl_auth("C:/Users/Youngjun/Google Drive/Google_cloud/translateR-079f74265489.json")
library(googleLanguageR)
library(googleLanguageR)
gl_auth("C:/Users/Youngjun/Google Drive/Google_cloud/translateR-079f74265489.json")
result1 <- mutate(result, Korean=gl_translate(subject,target="ko"))
library(dplyr)
result1 <- mutate(result, Korean=gl_translate(subject,target="ko"))
str(result)
result$subject <- as.character(result$subject)
str(result)
result1 <- mutate(result, Korean=gl_translate(subject,target="ko"))
result1 <- mutate(result, Korean=gl_translate(subject,target="ko", format="html"))
gl_translate(result$subject[2], target = "ko", format="html")
gl_translate(result$subject[1], target = "ko", format="html")
gl_translate(result$subject[1], target = "ko")
a <- gl_translate(result$subject[1], target = "ko")
View(a)
a <- gl_translate(result$subject[1], target = "ko")[1]
View(a)
result <- cbind(result, Korean)
result <- cbind(result, Korean=NA)
View(result)
result <- read.csv("journal_result_2017_12.txt")
result$subject <- as.character(result$subject)
result <- cbind(result, google_translate=NA)
n <- nrow(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[1], target = "ko")[1]
}
View(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
View(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
result$subject[1]
result$subject[2]
result$subject[3]
result$subject[4]
#.Renviron
GL_AUTH="C:/Users/Youngjun/Google Drive/Google_cloud/translateR-079f74265489.json"
library(googleLanguageR)
gl_auth("C:/Users/Youngjun/Google Drive/Google_cloud/translateR-079f74265489.json")
result <- read.csv("journal_result_2017_12.txt")
result$subject <- as.character(result$subject)
result <- cbind(result, google_translate=NA)
n <- nrow(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
result$subject[1]
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
gl_translate(result$subject[1], target = "ko")[1]
gl_translate(result$subject[2], target = "ko")[1]
gl_translate(result$subject[3], target = "ko")[1]
for(i in 0:(n-1)){
i+1
result[i,9] <- gl_translate(result[i,6], target = "ko")[1]
}
result[1,6]
for(i in 0:(n-1)){
i+1
result[i,9] <- gl_translate(result[i,7], target = "ko")[1]
}
str(result)
result$subject <- as.vector(result$subject)
str(result)
result <- read.csv("journal_result_2017_12.txt")
result$subject <- as.vector(result$subject)
str(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
for(i in 0:(n-1)){
i+1
gl_translate(result$subject[i], target = "ko")[1]
}
nchar(result$subject[1])
nchar(result$subject[2])
nchar(result$subject[3])
nchar(result$subject[4])
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- nchar(result$subject[i])
}
View(result)
for(i in 0:(n-1)){
i+1
result$google_translate[i] <- gl_translate(result$subject[i], target = "ko")[1]
}
gl_translate(result$subject[3], target = "ko")[1]
gl_translate(result$subject[11], target = "ko")[1]
google <- function(x){
gl_translate(x, target = "ko")[1]
}
google(result$subject[1])
a <- google(result$subject[1])
View(a)
for(i in 0:(n-1)){
i+1
x <- result$subject[i]
result$google_translate[i] <- google(x)
}
gl_translate(result$subject[11], target = "ko")[1]
for(i in 0:(n-1)){
i+1
result <- gl_translate(result$subject[i], target = "ko")
result$google_translate[i] <- result[1]
}
for(i in 0:(n-1)){
i+1
translated <- gl_translate(result$subject[i], target = "ko")
result$google_translate[i] <- translated[1]
}
gl_translate(result$subject[11], target = "ko")[1]
for(i in 0:(n-1)){
i+1
translated <- gl_translate(result$subject[i], target = "ko", format="html")
result$google_translate[i] <- translated[1]
}
gl_translate(result$subject[11], target = "ko", model="base")[1]
gl_translate(result$subject[11], target = "ko", model="nmt")[1]
gl_translate(result$subject[11], target = "ja", model="nmt")[1]
gl_translate(result$subject[11], target = "de", model="nmt")[1]
for(i in 0:(n-1)){
i+1
translated <- gl_translate(result$subject[i], target = "ko")
result$google_translate[i] <- translated[1]
}
for(i in 0:(n-1)){
i+1
translated <- nchar(result$subject[i])
result$google_translate[i] <- translated[1]
}
View(result)
dir()
xlsx_to_csv <- function(path){
xlsx <- readxl::read_excel(path)
write.csv(xlsx,path,row.names=FALSE)
}
xlsx_to_csv("journal_result_2017_12.xlsx")
read.csv("journal_result_2017_12.xlsx")
a <- read.csv("journal_result_2017_12.xlsx")
View(a)
xlsx_to_csv <- function(path,csvname="result_csv.txt"){
xlsx <- readxl::read_excel(path)
write.csv(xlsx,csvname,row.names=FALSE)
}
xlsx_to_csv("journal_result_2017_12.xlsx")
View(a)
xlsx_to_csv("journal_result_2017_12.xlsx")
